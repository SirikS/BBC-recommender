{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283367aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbdf925",
   "metadata": {},
   "source": [
    "# Assign helpfull id's and process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c6e8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/webscraped_raw.csv', sep=';')\n",
    "\n",
    "# reformat date correctly\n",
    "data['date'] = pd.to_datetime(data.Date, errors='coerce').dt.date\n",
    "\n",
    "# substract season and episode number\n",
    "data['Season_no'] = data['Season'].str.extract('(\\d+)').astype('float').astype('Int64')\n",
    "data['Episode_no'] = data['Episode'].str.extract('(\\d+)').astype('float').astype('Int64')\n",
    "\n",
    "# select top genre per season because there are double entries\n",
    "show_genre = data.groupby(['Title', 'Genre']).count().Title_all.reset_index()\n",
    "show_genre = show_genre.sort_values('Title_all').groupby('Title').first().reset_index(\n",
    "                                    )[['Title', 'Genre']].rename({'Genre':'genre'}, axis=1)\n",
    "data = data.merge(show_genre, on='Title')\n",
    "\n",
    "# drop duplicate entries\n",
    "data = data.drop_duplicates(subset=['Title', 'Season+Episode'], keep='first')\n",
    "\n",
    "# assign Show_ID, Content_ID and Episode_ID (within a show)\n",
    "data['Content_ID'] = data.index\n",
    "season_id = pd.Series(data['Title'].unique()).rename('Title').reset_index().rename({'index':'Show_ID'}, axis=1)\n",
    "data = data.merge(season_id, on='Title')\n",
    "data['Episode_ID'] = data.sort_values(['Season_no', 'Episode_no']).groupby('Show_ID').cumcount() + 1\n",
    "\n",
    "# select columns and rename\n",
    "data = data[['Show_ID', 'Title', 'Content_ID', 'Episode_ID','Season_no', 'Episode_no', 'Season+Episode', \n",
    "             'Episode', 'genre', 'DurationMin', 'date', 'Year', 'Description', 'Image']]\n",
    "data = data.rename({'season_no':'season','Episode_no':'Episode', \n",
    "             'Episode':'Episode_name', 'DurationMin':'Duration', 'date':'Date', 'genre':'Genre'}, axis=1)\n",
    "data = data.fillna(value={'Season_no':'Other', 'Episode':'Unkown'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48ea95f",
   "metadata": {},
   "source": [
    "# Cluster on description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74a55ee6-c7cd-4595-baae-a3fb9abd410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "texts = data.Description.values\n",
    "\n",
    "#process all the descriptions\n",
    "processed_texts = [text for text in nlp.pipe(texts, disable=[\"ner\",\"parser\"])]\n",
    "\n",
    "# tokenize text, use lemmatized words, without stopwords or punctuation. \n",
    "tokenized_texts = [[word.lemma_ for word in processed_text\n",
    "                                if not word.is_stop and not word.is_punct]\n",
    "                                for processed_text in processed_texts]\n",
    "\n",
    "strings = [[' '.join([str(w) for w in tokenized_text])] for tokenized_text in tokenized_texts]\n",
    "\n",
    "for i in range(len(strings)):\n",
    "    strings[i] = strings[i][0]\n",
    "data['tokenized_text'] = strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d3028e2-cfaa-4aef-9459-fd46c5ede980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a new smaller dataset, and merge the tokenized text on a per-show basis\n",
    "df_pershow = data[[\"Show_ID\", \"Title\", \"tokenized_text\"]]\n",
    "df_pershow = df_pershow.groupby(['Show_ID', 'Title'], as_index = False).agg({'tokenized_text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7880282-0091-4139-a4f5-23401896d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a TF_IDF vector\n",
    "vectorizer = TfidfVectorizer(min_df=3, max_df=0.9, norm='l2')\n",
    "X = vectorizer.fit_transform(df_pershow['tokenized_text'])\n",
    "tf_idf = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06e45cbb-670a-4a34-bfe5-4d46caf9ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clusters using K-means and TF-IDF for the similar shows recommendations\n",
    "clusters = 25\n",
    "kmeanModel = KMeans(n_clusters=clusters, init='k-means++', max_iter=3000, random_state=0)\n",
    "mod = kmeanModel.fit_transform(tf_idf)\n",
    "df_pershow['k_means'] = kmeanModel.predict(tf_idf)\n",
    "df_pershow = df_pershow[[\"Show_ID\", \"Title\", \"k_means\"]]\n",
    "\n",
    "merged_data = pd.merge(data, df_pershow,\n",
    "                        how=\"left\", on=[\"Show_ID\", \"Title\"])\n",
    "\n",
    "tf_idf_df = pd.DataFrame(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "03542fab-84a8-4f43-889a-48aa7ff5c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store processed data\n",
    "tf_idf_df.to_csv('../data/tfidf.csv', index=False)\n",
    "merged_data.to_csv('../data/BBC_episodes.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af38e276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
