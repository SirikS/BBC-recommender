{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "283367aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c6e8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/final_df.csv', sep=';')\n",
    "\n",
    "# reformat date correctly\n",
    "data['date'] = pd.to_datetime(data.Date, errors='coerce').dt.date\n",
    "\n",
    "# substract season and episode number\n",
    "data['Season_no'] = data['Season'].str.extract('(\\d+)')\n",
    "data['Episode_no'] = data['Episode'].str.extract('(\\d+)')\n",
    "\n",
    "# select top genre per season because there are double entries\n",
    "show_genre = data.groupby(['Title', 'Genre']).count().Title_all.reset_index()\n",
    "show_genre = show_genre.sort_values('Title_all').groupby('Title').first().reset_index(\n",
    "                                    )[['Title', 'Genre']].rename({'Genre':'genre'}, axis=1)\n",
    "data = data.merge(show_genre, on='Title')\n",
    "\n",
    "# drop duplicate entries\n",
    "data = data.drop_duplicates(subset=['Title', 'Season+Episode'], keep='first')\n",
    "\n",
    "# assign Show_ID, Content_ID and Episode_ID (within a show)\n",
    "data['Content_ID'] = data.index\n",
    "season_id = pd.Series(data['Title'].unique()).rename('Title').reset_index().rename({'index':'Show_ID'}, axis=1)\n",
    "data = data.merge(season_id, on='Title')\n",
    "data['Episode_ID'] = data.sort_values(['Season_no', 'Episode']).groupby('Show_ID').cumcount() + 1\n",
    "\n",
    "# select columns and rename\n",
    "data = data[['Show_ID', 'Title', 'Content_ID', 'Episode_ID','Season_no', 'Episode_no', 'Season+Episode', \n",
    "             'Episode', 'genre', 'DurationMin', 'date', 'Year', 'Description', 'Image']]\n",
    "data = data.rename({'season_no':'season','Episode_no':'Episode', \n",
    "             'Episode':'Episode_name', 'DurationMin':'Duration', 'date':'Date', 'genre':'Genre'}, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7495bce1-791e-41ea-af07-f0d0bd968dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data.Description.values\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74a55ee6-c7cd-4595-baae-a3fb9abd410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process all the descriptions\n",
    "\n",
    "processed_texts = [text for text in nlp.pipe(texts, \n",
    "                                              disable=[\"ner\",\n",
    "                                                       \"parser\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbaf3d64-095a-48be-b5a3-50ec160ff63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize text, I use lemmatized words, without stopwords or punctuation. \n",
    "tokenized_texts = [[word.lemma_ for word in processed_text\n",
    "                                if not word.is_stop and not word.is_punct]\n",
    "                                for processed_text in processed_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e9892b3-05b9-4b5d-9927-3d1526a7c7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'painting unknown man unknown artist turn great'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "strings = [[' '.join([str(w) for w in tokenized_text])] for tokenized_text in tokenized_texts]\n",
    "\n",
    "for i in range(len(strings)):\n",
    "    strings[i] = strings[i][0]\n",
    "data['tokenized_text'] = strings\n",
    "data.tokenized_text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d3028e2-cfaa-4aef-9459-fd46c5ede980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new smaller dataset, and merge the tokenized text on a per-show basis\n",
    "df_pershow = data[[\"Show_ID\", \"Title\", \"tokenized_text\"]]\n",
    "df_pershow = df_pershow.groupby(['Show_ID', 'Title'], as_index = False).agg({'tokenized_text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1499c4cf-a9dd-4c77-9270-880e45f1177a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7880282-0091-4139-a4f5-23401896d5e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>13th</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>150</th>\n",
       "      <th>...</th>\n",
       "      <th>zeinab</th>\n",
       "      <th>zero</th>\n",
       "      <th>zimbabwe</th>\n",
       "      <th>zip</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zuu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1484 rows × 6179 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      000   10  100   11   12   13  13th   14   15  150  ...  zeinab  zero  \\\n",
       "0     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  ...     0.0   0.0   \n",
       "1     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  ...     0.0   0.0   \n",
       "2     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  ...     0.0   0.0   \n",
       "3     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  ...     0.0   0.0   \n",
       "4     0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  ...     0.0   0.0   \n",
       "...   ...  ...  ...  ...  ...  ...   ...  ...  ...  ...  ...     ...   ...   \n",
       "1479  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  ...     0.0   0.0   \n",
       "1480  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  ...     0.0   0.0   \n",
       "1481  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  ...     0.0   0.0   \n",
       "1482  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  ...     0.0   0.0   \n",
       "1483  0.0  0.0  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  ...     0.0   0.0   \n",
       "\n",
       "      zimbabwe  zip  zoe  zombie  zone  zoo  zoom  zuu  \n",
       "0          0.0  0.0  0.0     0.0   0.0  0.0   0.0  0.0  \n",
       "1          0.0  0.0  0.0     0.0   0.0  0.0   0.0  0.0  \n",
       "2          0.0  0.0  0.0     0.0   0.0  0.0   0.0  0.0  \n",
       "3          0.0  0.0  0.0     0.0   0.0  0.0   0.0  0.0  \n",
       "4          0.0  0.0  0.0     0.0   0.0  0.0   0.0  0.0  \n",
       "...        ...  ...  ...     ...   ...  ...   ...  ...  \n",
       "1479       0.0  0.0  0.0     0.0   0.0  0.0   0.0  0.0  \n",
       "1480       0.0  0.0  0.0     0.0   0.0  0.0   0.0  0.0  \n",
       "1481       0.0  0.0  0.0     0.0   0.0  0.0   0.0  0.0  \n",
       "1482       0.0  0.0  0.0     0.0   0.0  0.0   0.0  0.0  \n",
       "1483       0.0  0.0  0.0     0.0   0.0  0.0   0.0  0.0  \n",
       "\n",
       "[1484 rows x 6179 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a TF_IDF vector\n",
    "vectorizer = TfidfVectorizer(min_df=3, max_df=0.9, norm='l2')\n",
    "X = vectorizer.fit_transform(df_pershow['tokenized_text'])\n",
    "tf_idf = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names())\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06e45cbb-670a-4a34-bfe5-4d46caf9ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clusters using K-means and TF-IDF for the similar shows recommendations\n",
    "clusters = 25\n",
    "kmeanModel = KMeans(n_clusters=clusters, init='k-means++', max_iter=3000, random_state=0)\n",
    "mod = kmeanModel.fit_transform(tf_idf)\n",
    "df_pershow['k_means'] = kmeanModel.predict(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9713b1a6-d3e1-412d-860b-6a779f01627e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, story, sea, anne, west, people, tell, east, crew, explore, fire, read, league, pandemic, ruth, wildlife, premier, dougie, mark, barra, obama, \n",
      "1, find, life, new, try, face, family, look, take, meet, queen, reveal, late, challenge, home, question, live, comedy, peter, change, man, \n",
      "2, political, debate, interview, late, news, mp, guest, elizabeth, reynolds, amelia, tim, 1970, jane, james, williams, peter, leader, activity, plus, wales, \n",
      "3, rick, look, london, world, year, british, series, 1989, 1978, travel, louis, britain, adam, monty, black, fashion, interior, documentary, old, follow, \n",
      "4, explore, journey, travel, meet, river, bikers, robert, paul, stacey, ireland, visit, reggie, coast, mary, continue, bob, northern, life, group, macdonald, \n",
      "5, simon, reeve, travel, journey, mountains, leg, peninsula, aegean, explore, visit, begin, national, follow, cornwall, belonging, coast, glorious, world, island, kenya, \n",
      "6, good, zoo, series, itã, bit, radio, day, dom, rhod, katy, boy, driver, build, animal, moment, dish, new, howard, conversation, room, \n",
      "7, film, talk, festival, sir, wheeler, mortimer, 1966, british, story, john, life, report, berlin, black, art, work, maker, rise, hughes, world, \n",
      "8, david, attenborough, cornock, parliament, look, highlight, animal, olusoga, world, frost, present, ocean, 1960, sir, story, wilson, life, reveal, june, interview, \n",
      "9, coverage, live, cup, final, present, championships, match, 2022, swpl, league, city, wales, women, round, irish, highlight, watson, womenã, fa, stephen, \n",
      "10, chris, brian, steve, cox, packham, backshall, animal, planet, earth, shark, professor, explore, michaela, stark, reveal, drama, natural, sing, pui, large, \n",
      "11, friend, find, play, help, get, want, sing, try, day, mr, song, tumble, garden, little, big, new, way, make, time, thing, \n",
      "12, music, bbc, perform, 2021, michael, palin, glasgow, awards, festival, highlight, live, singer, tv, look, session, present, compete, performance, jones, radio, \n",
      "13, murder, investigate, case, investigation, man, killer, team, find, police, evidence, death, victim, woman, body, trial, young, detective, uncover, year, dead, \n",
      "14, guest, join, miriam, panel, mp, margolyes, special, damian, wogan, sir, include, hotel, talk, michelle, nick, darren, matt, andrew, terry, dougie, \n",
      "15, year, school, old, new, student, learn, mr, pupil, joe, class, help, change, life, try, day, people, look, world, lesson, ben, \n",
      "16, recipe, include, nigella, martin, cook, lorraine, sketch, feature, nigel, share, nadiya, lazy, chicken, subject, cake, 2002, make, menu, sweet, paul, \n",
      "17, dr, judge, cook, chef, celebrate, best, dish, winner, final, ultimate, sam, join, barra, mark, xand, studio, finalist, celebrity, week, bake, \n",
      "18, team, alan, transform, game, quiz, help, feature, beat, restore, stacey, play, present, ross, family, expert, home, talk, change, build, candidate, \n",
      "19, andy, look, adventure, head, year, search, mark, million, scottish, past, fred, lance, travel, time, 2019, anthony, shaky, tourism, ruby, clark, \n",
      "20, band, gig, industry, big, josh, night, head, mate, designer, bryn, christine, wembley, wonderland, prepare, tam, louis, blocks, come, meet, van, \n",
      "21, james, cat, yn, help, surgery, life, dog, vet, rory, charley, pup, mouse, surgeon, ac, sandy, 2012, kit, origin, hannah, delaney, \n",
      "22, new, gang, plan, find, family, try, face, couple, life, help, truth, secret, get, day, tom, discover, come, friend, decide, house, \n",
      "23, join, maddie, alex, highlight, maya, royal, learn, wet, rob, scott, queen, sara, jones, ed, tee, star, corps, find, present, sam, \n",
      "24, march, question, tuesday, minister, commons, wednesday, parliament, statement, coverage, lords, scottish, thursday, house, westminster, monday, proceeding, debate, bill, february, committee, \n"
     ]
    }
   ],
   "source": [
    "order_centroids = kmeanModel.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "dict = []\n",
    "for i in range(clusters):\n",
    "    print('%d' % i, sep='', end=', '),\n",
    "    for ind in order_centroids[i, :20]:\n",
    "        print(terms[ind], sep='', end=', ')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aac84a4-99ca-41a4-9643-62ffd5bf127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pershow = df_pershow[[\"Show_ID\", \"Title\", \"k_means\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90679030-a8ec-4c6d-9487-d777c4707605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show_ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content_ID</th>\n",
       "      <th>Episode_ID</th>\n",
       "      <th>Season_no</th>\n",
       "      <th>Episode</th>\n",
       "      <th>Season+Episode</th>\n",
       "      <th>Episode_name</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Description</th>\n",
       "      <th>Image</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>k_means</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A Timewatch Guide</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Series 3: 2. Women, Sex and Society</td>\n",
       "      <td>2. Women, Sex and Society</td>\n",
       "      <td>history</td>\n",
       "      <td>59</td>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>2016</td>\n",
       "      <td>How the transformation of the rights and role ...</td>\n",
       "      <td>https://ichef.bbci.co.uk/images/ic/1200x675/p0...</td>\n",
       "      <td>transformation right role woman document telev...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Britain's Lost Masterpieces</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>Series 2: 4. Arbroath</td>\n",
       "      <td>4. Arbroath</td>\n",
       "      <td>signed</td>\n",
       "      <td>59</td>\n",
       "      <td>2017-10-18</td>\n",
       "      <td>2017</td>\n",
       "      <td>A painting of an unknown man by an unknown art...</td>\n",
       "      <td>https://ichef.bbci.co.uk/images/ic/1200x675/p0...</td>\n",
       "      <td>painting unknown man unknown artist turn great</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Britain's Lost Masterpieces</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Series 5: 1. Brighton</td>\n",
       "      <td>1. Brighton</td>\n",
       "      <td>signed</td>\n",
       "      <td>59</td>\n",
       "      <td>2021-02-01</td>\n",
       "      <td>2021</td>\n",
       "      <td>Bendor Grosvenor and Emma Dabiri investigate t...</td>\n",
       "      <td>https://ichef.bbci.co.uk/images/ic/1200x675/p0...</td>\n",
       "      <td>Bendor Grosvenor Emma Dabiri investigate paint...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Britain's Lost Masterpieces</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>Series 5: 2. Tatton Park</td>\n",
       "      <td>2. Tatton Park</td>\n",
       "      <td>signed</td>\n",
       "      <td>59</td>\n",
       "      <td>2021-02-08</td>\n",
       "      <td>2021</td>\n",
       "      <td>Bendor and Emma discover a mysterious portrait...</td>\n",
       "      <td>https://ichef.bbci.co.uk/images/ic/1200x675/p0...</td>\n",
       "      <td>Bendor Emma discover mysterious portrait 16th ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Britain's Lost Masterpieces</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>Series 5: 3. Glasgow</td>\n",
       "      <td>3. Glasgow</td>\n",
       "      <td>signed</td>\n",
       "      <td>59</td>\n",
       "      <td>2022-02-07</td>\n",
       "      <td>2022</td>\n",
       "      <td>Technical problems frustrate Bendor and EmmaÃ...</td>\n",
       "      <td>https://ichef.bbci.co.uk/images/ic/1200x675/p0...</td>\n",
       "      <td>technical problem frustrate Bendor EmmaÃÂ¢Ã¢Â...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21684</th>\n",
       "      <td>1481</td>\n",
       "      <td>GAA Live</td>\n",
       "      <td>34785</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Armagh v Kildare</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sports</td>\n",
       "      <td>119</td>\n",
       "      <td>2022-03-12</td>\n",
       "      <td>2022</td>\n",
       "      <td>Live coverage of the Division One GAA football...</td>\n",
       "      <td>https://ichef.bbci.co.uk/images/ic/1200x675/p0...</td>\n",
       "      <td>live coverage Division GAA football match Arma...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21685</th>\n",
       "      <td>1481</td>\n",
       "      <td>GAA Live</td>\n",
       "      <td>34786</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Down v Kerry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sports</td>\n",
       "      <td>180</td>\n",
       "      <td>2022-03-19</td>\n",
       "      <td>2022</td>\n",
       "      <td>Live coverage of the Division Two GAA hurling ...</td>\n",
       "      <td>https://ichef.bbci.co.uk/images/ic/1200x675/p0...</td>\n",
       "      <td>live coverage Division GAA hurling match Kerry</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21686</th>\n",
       "      <td>1482</td>\n",
       "      <td>BBC Sport NI</td>\n",
       "      <td>34787</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ulster Schools' Cup Rugby Final</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sports</td>\n",
       "      <td>120</td>\n",
       "      <td>2022-03-17</td>\n",
       "      <td>2022</td>\n",
       "      <td>Live coverage of the 2022 SchoolsÃÂ¢Ã¢ÂÂ¬Ã¢Â...</td>\n",
       "      <td>https://ichef.bbci.co.uk/images/ic/1200x675/p0...</td>\n",
       "      <td>live coverage 2022 SchoolsÃÂ¢Ã¢ÂÂ¬Ã¢ÂÂ¢ Cup...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21687</th>\n",
       "      <td>1483</td>\n",
       "      <td>The Football News Show</td>\n",
       "      <td>34788</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22/03/2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sports</td>\n",
       "      <td>12</td>\n",
       "      <td>2022-03-22</td>\n",
       "      <td>2022</td>\n",
       "      <td>We focus on fans looking to make a change in f...</td>\n",
       "      <td>https://ichef.bbci.co.uk/images/ic/1200x675/p0...</td>\n",
       "      <td>focus fan look change football plus woman Cham...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21688</th>\n",
       "      <td>1483</td>\n",
       "      <td>The Football News Show</td>\n",
       "      <td>34789</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23/03/2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sports</td>\n",
       "      <td>11</td>\n",
       "      <td>2022-03-23</td>\n",
       "      <td>2022</td>\n",
       "      <td>We focus on Manchester United speaking to Ajax...</td>\n",
       "      <td>https://ichef.bbci.co.uk/images/ic/1200x675/p0...</td>\n",
       "      <td>focus Manchester United speak Ajax Erik Hag ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21689 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Show_ID                        Title  Content_ID  Episode_ID Season_no  \\\n",
       "0            0            A Timewatch Guide           0           1         3   \n",
       "1            1  Britain's Lost Masterpieces           3           1         2   \n",
       "2            1  Britain's Lost Masterpieces           4           2         5   \n",
       "3            1  Britain's Lost Masterpieces           5           3         5   \n",
       "4            1  Britain's Lost Masterpieces           6           4         5   \n",
       "...        ...                          ...         ...         ...       ...   \n",
       "21684     1481                     GAA Live       34785           1       NaN   \n",
       "21685     1481                     GAA Live       34786           2       NaN   \n",
       "21686     1482                 BBC Sport NI       34787           1       NaN   \n",
       "21687     1483       The Football News Show       34788           1       NaN   \n",
       "21688     1483       The Football News Show       34789           2       NaN   \n",
       "\n",
       "      Episode                       Season+Episode  \\\n",
       "0           2  Series 3: 2. Women, Sex and Society   \n",
       "1           4                Series 2: 4. Arbroath   \n",
       "2           1                Series 5: 1. Brighton   \n",
       "3           2             Series 5: 2. Tatton Park   \n",
       "4           3                 Series 5: 3. Glasgow   \n",
       "...       ...                                  ...   \n",
       "21684     NaN                     Armagh v Kildare   \n",
       "21685     NaN                         Down v Kerry   \n",
       "21686     NaN      Ulster Schools' Cup Rugby Final   \n",
       "21687     NaN                           22/03/2022   \n",
       "21688     NaN                           23/03/2022   \n",
       "\n",
       "                     Episode_name    Genre  Duration        Date  Year  \\\n",
       "0       2. Women, Sex and Society  history        59  2016-11-15  2016   \n",
       "1                     4. Arbroath   signed        59  2017-10-18  2017   \n",
       "2                     1. Brighton   signed        59  2021-02-01  2021   \n",
       "3                  2. Tatton Park   signed        59  2021-02-08  2021   \n",
       "4                      3. Glasgow   signed        59  2022-02-07  2022   \n",
       "...                           ...      ...       ...         ...   ...   \n",
       "21684                         NaN   sports       119  2022-03-12  2022   \n",
       "21685                         NaN   sports       180  2022-03-19  2022   \n",
       "21686                         NaN   sports       120  2022-03-17  2022   \n",
       "21687                         NaN   sports        12  2022-03-22  2022   \n",
       "21688                         NaN   sports        11  2022-03-23  2022   \n",
       "\n",
       "                                             Description  \\\n",
       "0      How the transformation of the rights and role ...   \n",
       "1      A painting of an unknown man by an unknown art...   \n",
       "2      Bendor Grosvenor and Emma Dabiri investigate t...   \n",
       "3      Bendor and Emma discover a mysterious portrait...   \n",
       "4      Technical problems frustrate Bendor and EmmaÃ...   \n",
       "...                                                  ...   \n",
       "21684  Live coverage of the Division One GAA football...   \n",
       "21685  Live coverage of the Division Two GAA hurling ...   \n",
       "21686  Live coverage of the 2022 SchoolsÃÂ¢Ã¢ÂÂ¬Ã¢Â...   \n",
       "21687  We focus on fans looking to make a change in f...   \n",
       "21688  We focus on Manchester United speaking to Ajax...   \n",
       "\n",
       "                                                   Image  \\\n",
       "0      https://ichef.bbci.co.uk/images/ic/1200x675/p0...   \n",
       "1      https://ichef.bbci.co.uk/images/ic/1200x675/p0...   \n",
       "2      https://ichef.bbci.co.uk/images/ic/1200x675/p0...   \n",
       "3      https://ichef.bbci.co.uk/images/ic/1200x675/p0...   \n",
       "4      https://ichef.bbci.co.uk/images/ic/1200x675/p0...   \n",
       "...                                                  ...   \n",
       "21684  https://ichef.bbci.co.uk/images/ic/1200x675/p0...   \n",
       "21685  https://ichef.bbci.co.uk/images/ic/1200x675/p0...   \n",
       "21686  https://ichef.bbci.co.uk/images/ic/1200x675/p0...   \n",
       "21687  https://ichef.bbci.co.uk/images/ic/1200x675/p0...   \n",
       "21688  https://ichef.bbci.co.uk/images/ic/1200x675/p0...   \n",
       "\n",
       "                                          tokenized_text  k_means  \n",
       "0      transformation right role woman document telev...        1  \n",
       "1         painting unknown man unknown artist turn great        1  \n",
       "2      Bendor Grosvenor Emma Dabiri investigate paint...        1  \n",
       "3      Bendor Emma discover mysterious portrait 16th ...        1  \n",
       "4      technical problem frustrate Bendor EmmaÃÂ¢Ã¢Â...        1  \n",
       "...                                                  ...      ...  \n",
       "21684  live coverage Division GAA football match Arma...        9  \n",
       "21685     live coverage Division GAA hurling match Kerry        9  \n",
       "21686  live coverage 2022 SchoolsÃÂ¢Ã¢ÂÂ¬Ã¢ÂÂ¢ Cup...        9  \n",
       "21687  focus fan look change football plus woman Cham...        1  \n",
       "21688  focus Manchester United speak Ajax Erik Hag ma...        1  \n",
       "\n",
       "[21689 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_data = pd.merge(data, df_pershow,\n",
    "                        how=\"left\", on=[\"Show_ID\", \"Title\"])\n",
    "merged_data.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03542fab-84a8-4f43-889a-48aa7ff5c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df = pd.DataFrame(tf_idf)\n",
    "tf_idf_df.to_csv('../data/tfidf.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3fe8875-359f-4106-9413-2771f3ba2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store processed data\n",
    "merged_data.to_csv('../data/BBC_episodes.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
